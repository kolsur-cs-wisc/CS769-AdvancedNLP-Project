Some weights of ViltForQuestionAnswering were not initialized from the model checkpoint at dandelin/vilt-b32-mlm and are newly initialized: ['classifier.3.bias', 'classifier.3.weight', 'classifier.0.bias', 'classifier.1.bias', 'classifier.0.weight', 'classifier.1.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch: 0
  0%|          | 0/81 [00:00<?, ?it/s]
Loss: 24.005281448364258
Loss: 4.113085746765137
Loss: 4.355993270874023
Loss: 2.7222867012023926
Loss: 4.2720465660095215
Loss: 3.4346368312835693
Loss: 4.422051906585693
Loss: 2.803269863128662
Loss: 2.6381218433380127
Loss: 4.090445518493652
Loss: 4.993913650512695
Loss: 2.3507606983184814
Loss: 2.789050579071045
Loss: 3.6531736850738525
Loss: 2.916597366333008
Loss: 2.3513660430908203
Loss: 3.0681164264678955
Loss: 2.9998083114624023
Loss: 2.1948063373565674
Loss: 2.6105144023895264
Loss: 2.8010175228118896
Loss: 2.7112913131713867
Loss: 3.452333450317383
Loss: 3.1642050743103027
Loss: 2.085911750793457
Loss: 2.407548666000366
Loss: 1.8025131225585938
Loss: 2.6620163917541504
Loss: 2.206881046295166
Loss: 2.2907989025115967
Loss: 2.175464391708374
Loss: 2.521712064743042
Loss: 2.37288761138916
Loss: 2.1190032958984375
Loss: 2.355766773223877
Loss: 2.1779160499572754
Loss: 3.3044629096984863
Loss: 2.062559127807617
Loss: 1.8327096700668335
Loss: 2.0145211219787598
Loss: 2.169313430786133
Loss: 3.5403189659118652
Loss: 2.3413217067718506
Loss: 2.0643539428710938
Loss: 2.3920319080352783
Loss: 2.4638776779174805
Loss: 2.4925148487091064
Loss: 1.9099229574203491
Loss: 3.2842583656311035
Loss: 2.549332857131958
Loss: 2.0597689151763916
Loss: 2.5016045570373535
Loss: 2.55197811126709
Loss: 2.080259323120117
Loss: 1.8640023469924927
Loss: 1.740792989730835
Loss: 2.683065891265869
Loss: 3.072692394256592
Loss: 1.8512370586395264
Loss: 2.772181510925293
Loss: 1.9200366735458374
Loss: 2.499227285385132
Loss: 1.6950187683105469
Loss: 2.5066537857055664
Loss: 2.499640941619873
Loss: 2.328169584274292
Loss: 2.2960398197174072
Loss: 1.9960743188858032
Loss: 2.173663854598999
Loss: 2.6728453636169434
Loss: 1.7729511260986328
Loss: 2.0554912090301514
Loss: 2.627199411392212
Loss: 2.0620529651641846
Loss: 2.4714407920837402
Loss: 3.314776659011841
Loss: 3.4369423389434814
Loss: 1.8881824016571045
Loss: 2.427682399749756
Loss: 1.9761285781860352
Loss: 2.2626209259033203
  0%|          | 0/53 [00:00<?, ?it/s]
Validate Accuracy:  tensor(0.4313, device='cuda:3')
Epoch: 1
  0%|          | 0/81 [00:00<?, ?it/s]
Loss: 1.8738787174224854
Loss: 2.218024492263794
Loss: 2.5759713649749756
Loss: 2.8121612071990967
Loss: 1.9316346645355225
Loss: 2.013643980026245
Loss: 2.2656359672546387
Loss: 2.3676564693450928
Loss: 2.3408286571502686
Loss: 1.8140016794204712
Loss: 1.9475177526474
Loss: 3.2660934925079346
Loss: 2.6554081439971924
Loss: 2.614694833755493
Loss: 2.2405412197113037
Loss: 1.806203842163086
Loss: 1.8402855396270752
Loss: 2.131110668182373
Loss: 2.2669079303741455
Loss: 1.8333779573440552
Loss: 1.892430067062378
Loss: 1.9557971954345703
Loss: 1.8712668418884277
Loss: 2.5291216373443604
Loss: 2.573146104812622
Loss: 2.0159213542938232
Loss: 1.483218789100647
Loss: 2.1196517944335938
Loss: 1.5349664688110352
Loss: 2.1255366802215576
Loss: 2.4117848873138428
Loss: 2.139810800552368
Loss: 3.115708112716675
Loss: 1.8575681447982788
Loss: 2.930418014526367
Loss: 2.4317824840545654
Loss: 1.953744888305664
Loss: 1.9744985103607178
Loss: 1.9691396951675415
Loss: 2.502258539199829
Loss: 2.984170436859131
Loss: 1.5385520458221436
Loss: 2.3838748931884766
Loss: 1.808183193206787
Loss: 2.2568750381469727
Loss: 1.826305627822876
Loss: 2.624830722808838
Loss: 2.254539966583252
Loss: 2.2247979640960693
Loss: 1.999585509300232
Loss: 1.8673372268676758
Loss: 2.6026792526245117
Loss: 2.6366677284240723
Loss: 2.098773241043091
Loss: 2.202289581298828
Loss: 2.811868667602539
Loss: 2.7073161602020264
Loss: 2.444580554962158
Loss: 1.9566221237182617
Loss: 2.5363879203796387
Loss: 2.3350884914398193
Loss: 2.0633044242858887
Loss: 2.0761353969573975
Loss: 2.452850341796875
Loss: 1.9269275665283203
Loss: 2.2480294704437256
Loss: 2.29270339012146
Loss: 1.4850233793258667
Loss: 2.2462339401245117
Loss: 2.1419341564178467
Loss: 2.5252225399017334
Loss: 1.9545785188674927
Loss: 1.9279088973999023
Loss: 1.931807279586792
Loss: 2.1844589710235596
Loss: 2.6032915115356445
Loss: 3.121009349822998
Loss: 2.4301106929779053
Loss: 2.291936159133911
Loss: 2.4783847332000732
Loss: 2.3409175872802734
  0%|          | 0/53 [00:00<?, ?it/s]
Validate Accuracy:  tensor(0.4171, device='cuda:3')
Epoch: 2
  0%|          | 0/81 [00:00<?, ?it/s]
Loss: 2.886547803878784
Loss: 2.3440732955932617
Loss: 2.793975353240967
Loss: 1.8464263677597046
Loss: 1.9786688089370728
Loss: 2.2062995433807373
Loss: 2.347121477127075
Loss: 2.334413766860962
Loss: 2.5076818466186523
Loss: 2.171234607696533
Loss: 2.4307608604431152
Loss: 2.6317667961120605
Loss: 1.675127387046814
Loss: 2.068357467651367
Loss: 1.8942224979400635
Loss: 1.928516149520874
Loss: 2.7660579681396484
Loss: 1.9754304885864258
Loss: 2.289267063140869
Loss: 2.2529425621032715
Loss: 1.8500115871429443
Loss: 3.041956901550293
Loss: 2.517273187637329
Loss: 2.5905630588531494
Loss: 2.4058306217193604
Loss: 2.1594831943511963
Loss: 2.4667415618896484
Loss: 2.318866729736328
Loss: 2.0849854946136475
Loss: 2.223923444747925
Loss: 1.850448727607727
Loss: 1.7892814874649048
Loss: 2.289292812347412
Loss: 1.7265383005142212
Loss: 1.9927289485931396
Loss: 2.519425392150879
Loss: 2.6169793605804443
Loss: 2.711805582046509
Loss: 1.7022368907928467
Loss: 2.1352288722991943
Loss: 1.6159075498580933
Loss: 1.625838279724121
Loss: 1.8774772882461548
Loss: 2.9717395305633545
Loss: 2.1543054580688477
Loss: 2.7068378925323486
Loss: 2.3085014820098877
Loss: 2.2772185802459717
Loss: 1.8216369152069092
Loss: 2.659327983856201
Loss: 2.1490375995635986
Loss: 2.1673216819763184
Loss: 2.1307294368743896
Loss: 2.1861412525177
Loss: 2.7536816596984863
Loss: 2.7349720001220703
Loss: 1.6921788454055786
Loss: 1.710852861404419
Loss: 1.8274930715560913
Loss: 1.8261483907699585
Loss: 2.473256826400757
Loss: 1.8228204250335693
Loss: 2.7749593257904053
Loss: 2.6531825065612793
Loss: 2.1278023719787598
Loss: 2.108745813369751
Loss: 1.9385930299758911
Loss: 2.0446317195892334
Loss: 1.9344072341918945
Loss: 2.989187717437744
Loss: 2.4906082153320312
Loss: 2.163964033126831
Loss: 1.6126117706298828
Loss: 1.841931939125061
Loss: 1.8069003820419312
Loss: 2.204719305038452
Loss: 2.750239372253418
Loss: 2.424168586730957
Loss: 1.7919169664382935
Loss: 2.8035149574279785
Loss: 2.0184555053710938
  0%|          | 0/53 [00:00<?, ?it/s]
Validate Accuracy:  tensor(0.4313, device='cuda:3')
Epoch: 3
  0%|          | 0/81 [00:00<?, ?it/s]
Loss: 2.385930299758911
Loss: 2.4037017822265625
Loss: 2.1486425399780273
Loss: 2.20943546295166
Loss: 2.2085208892822266
Loss: 2.681713819503784
Loss: 1.723710298538208
Loss: 2.133490562438965
Loss: 2.932065486907959
Loss: 1.7407335042953491
Loss: 2.309462070465088
Loss: 2.4021151065826416
Loss: 2.192627429962158
Loss: 2.4713642597198486
Loss: 1.820464849472046
Loss: 2.11678147315979
Loss: 1.8669732809066772
Loss: 2.1501846313476562
Loss: 2.770695924758911
Loss: 1.695374846458435
Loss: 1.939821720123291
Loss: 1.983114242553711
Loss: 2.490135669708252
Loss: 1.7923870086669922
Loss: 2.1447997093200684
Loss: 1.9718396663665771
Loss: 3.068154811859131
Loss: 1.6398115158081055
Loss: 1.4860508441925049
Loss: 2.2967782020568848
Loss: 2.2025208473205566
Loss: 2.527822494506836
Loss: 1.7856525182724
Loss: 2.604121685028076
Loss: 2.3442790508270264
Loss: 2.0606839656829834
Loss: 2.189194679260254
Loss: 2.1429340839385986
Loss: 2.4941389560699463
Loss: 2.439594268798828
Loss: 2.042952537536621
Loss: 1.9474103450775146
Loss: 1.58089017868042
Loss: 2.326951265335083
Loss: 2.158781051635742
Loss: 2.466263771057129
Loss: 1.699113130569458
Loss: 1.7384332418441772
Loss: 1.8595699071884155
Loss: 2.469820022583008
Loss: 2.6273117065429688
Loss: 1.91410231590271
Loss: 2.063331365585327
Loss: 2.8786461353302
Loss: 1.965165615081787
Loss: 2.1223247051239014
Loss: 2.1407928466796875
Loss: 2.4955384731292725
Loss: 2.641878128051758
Loss: 3.049656629562378
Loss: 2.444007158279419
Loss: 1.864215612411499
Loss: 2.247006416320801
Loss: 1.9929206371307373
Loss: 2.4108803272247314
Loss: 2.373789072036743
Loss: 2.219754934310913
Loss: 2.4570798873901367
Loss: 2.09794020652771
Loss: 1.5343551635742188
Loss: 2.2918508052825928
Loss: finetune_slake.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  true_labels = torch.tensor(batch['labels'].nonzero(as_tuple=True)[1])
finetune_slake.py:114: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  true_labels = torch.tensor(batch['labels'].nonzero(as_tuple=True)[1])
1.9168739318847656
Loss: 2.582937717437744
Loss: 2.0066256523132324
Loss: 2.2734215259552
Loss: 1.4883387088775635
Loss: 2.1465654373168945
Loss: 1.6645686626434326
Loss: 2.737898111343384
Loss: 2.364502191543579
Loss: 1.9871755838394165
  0%|          | 0/53 [00:00<?, ?it/s]
Validate Accuracy:  tensor(0.4313, device='cuda:3')
Epoch: 4
  0%|          | 0/81 [00:00<?, ?it/s]
Loss: 2.491365432739258
Loss: 2.5244476795196533
Loss: 1.5331554412841797
Loss: 2.9607300758361816
Loss: 2.2602713108062744
Loss: 1.9483777284622192
Loss: 1.952275276184082
Loss: 2.14117431640625
Loss: 2.2305338382720947
Loss: 2.2109785079956055
Loss: 2.171887159347534
Loss: 2.3704426288604736
Loss: 2.05911922454834
Loss: 2.5046145915985107
Loss: 1.9832830429077148
Loss: 1.9198647737503052
Loss: 1.8689053058624268
Loss: 1.9838969707489014
Loss: 2.3317222595214844
Loss: 2.3167850971221924
Loss: 2.065404176712036
Loss: 2.409364700317383
Loss: 2.6283228397369385
Loss: 2.3023905754089355
Loss: 2.652326822280884
Loss: 1.7399282455444336
Loss: 2.131744623184204
Loss: 2.2059755325317383
Loss: 2.2860121726989746
Loss: 2.055039405822754
Loss: 2.2749574184417725
Loss: 2.3892900943756104
Loss: 2.3796286582946777
Loss: 2.0130043029785156
Loss: 1.869269609451294
Loss: 1.7872860431671143
Loss: 2.303331136703491
Loss: 1.879293441772461
Loss: 2.9992809295654297
Loss: 1.9016404151916504
Loss: 2.2422711849212646
Loss: 2.86317777633667
Loss: 2.3140065670013428
Loss: 2.5443081855773926
Loss: 1.8851187229156494
Loss: 2.2639386653900146
Loss: 1.9897089004516602
Loss: 1.8407008647918701
Loss: 1.9094408750534058
Loss: 2.728518486022949
Loss: 2.010638475418091
Loss: 2.57196044921875
Loss: 2.001152753829956
Loss: 2.260941743850708
Loss: 2.1551713943481445
Loss: 2.1441104412078857
Loss: 2.0125839710235596
Loss: 1.9284675121307373
Loss: 2.098578453063965
Loss: 1.7046772241592407
Loss: 1.9385472536087036
Loss: 2.6281986236572266
Loss: 2.2048251628875732
Loss: 2.375133991241455
Loss: 2.6493403911590576
Loss: 1.9565824270248413
Loss: 2.425259590148926
Loss: 1.969752550125122
Loss: 2.4203238487243652
Loss: 2.362957715988159
Loss: 1.956916332244873
Loss: 2.208956480026245
Loss: 2.1236379146575928
Loss: 2.2165310382843018
Loss: 1.9531362056732178
Loss: 2.2222156524658203
Loss: 1.8823055028915405
Loss: 1.8576446771621704
Loss: 2.0772299766540527
Loss: 1.7576168775558472
Loss: 2.281799793243408
  0%|          | 0/53 [00:00<?, ?it/s]
Validate Accuracy:  tensor(0.4171, device='cuda:3')
  0%|          | 0/52 [00:00<?, ?it/s]
Test Accuracy:  tensor(0.4327, device='cuda:3')