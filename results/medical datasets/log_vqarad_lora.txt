Some weights of ViltForQuestionAnswering were not initialized from the model checkpoint at dandelin/vilt-b32-mlm and are newly initialized: ['classifier.3.bias', 'classifier.3.weight', 'classifier.1.weight', 'classifier.0.bias', 'classifier.1.bias', 'classifier.0.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch: 0
  0%|          | 0/44 [00:00<?, ?it/s]
Loss: 930.9470825195312
Loss: 1528.4969482421875
Loss: 16.68960189819336
Loss: 118.79981994628906
Loss: 2.7907626628875732
Loss: 1.6001695394515991
Loss: 2.9314892292022705
Loss: 3.3003783226013184
Loss: 3.1546859741210938
Loss: 2.4186081886291504
Loss: 3.1719353199005127
Loss: 2.1542322635650635
Loss: 3.313467264175415
Loss: 2.5998644828796387
Loss: 2.4179041385650635
Loss: 2.1291162967681885
Loss: 2.1117303371429443
Loss: 3.6867594718933105
Loss: 2.583286762237549
Loss: 3.169262647628784
Loss: 3.9216103553771973
Loss: 4.837627410888672
Loss: 2.047912836074829
Loss: 3.054612874984741
Loss: 2.847867727279663
Loss: 1.7358496189117432
Loss: 1.8634339570999146
Loss: 2.5600228309631348
Loss: 1.453820824623108
Loss: 2.513192653656006
Loss: 1.4623154401779175
Loss: 3.052582025527954
Loss: 2.9718024730682373
Loss: 2.988162040710449
Loss: 2.385002851486206
Loss: 1.8199125528335571
Loss: 1.8615385293960571
Loss: 2.5474042892456055
Loss: 2.772771120071411
Loss: 2.1593894958496094
Loss: 2.1919851303100586
Loss: 2.758908987045288
Loss: 2.141826868057251
Loss: 2.283942937850952
Epoch: 1
  0%|          | 0/44 [00:00<?, ?it/s]
Loss: 3.2189526557922363
Loss: 1.884113073348999
Loss: 2.1872353553771973
Loss: 1.9702669382095337
Loss: 2.003230571746826
Loss: 1.8042021989822388
Loss: 2.149216890335083
Loss: 2.313507318496704
Loss: 1.3894754648208618
Loss: 1.4658336639404297
Loss: 1.798143744468689
Loss: 2.5488052368164062
Loss: 2.3944151401519775
Loss: 1.4967851638793945
Loss: 1.9567646980285645
Loss: 2.936450958251953
Loss: 1.4315115213394165
Loss: 2.1072449684143066
Loss: 2.0695505142211914
Loss: 1.9381134510040283
Loss: 1.9027378559112549
Loss: 2.551774263381958
Loss: 1.847508430480957
Loss: 2.320072889328003
Loss: 2.1983375549316406
Loss: 1.9686356782913208
Loss: 1.533969521522522
Loss: 2.9164576530456543
Loss: 2.130206346511841
Loss: 2.1482841968536377
Loss: 1.82076096534729
Loss: 2.843674898147583
Loss: 2.8429794311523438
Loss: 2.4629065990448
Loss: 2.5236141681671143
Loss: 1.8698091506958008
Loss: 1.98537015914917
Loss: 2.126753568649292
Loss: 1.470436692237854
Loss: 2.4907500743865967
Loss: 2.107954978942871
Loss: 2.1922059059143066
Loss: 1.849959373474121
Loss: 1.5306246280670166
Epoch: 2
  0%|          | 0/44 [00:00<?, ?it/s]
Loss: 1.5984939336776733
Loss: 1.5807396173477173
Loss: 1.8099703788757324
Loss: 1.7994946241378784
Loss: 1.7998616695404053
Loss: 2.824453592300415
Loss: 1.7897132635116577
Loss: 1.7978010177612305
Loss: 1.4526121616363525
Loss: 1.7844524383544922
Loss: 2.48850154876709
Loss: 1.7530853748321533
Loss: 1.4566595554351807
Loss: 2.294358968734741
Loss: 2.3294413089752197
Loss: 2.5669941902160645
Loss: 2.1583168506622314
Loss: 2.2987277507781982
Loss: 2.3327364921569824
Loss: 2.20426869392395
Loss: 2.0480709075927734
Loss: 2.162862539291382
Loss: 2.4985313415527344
Loss: 2.1280746459960938
Loss: 1.8685495853424072
Loss: 2.6663098335266113
Loss: 2.4018304347991943
Loss: 2.164153575897217
Loss: 2.6656126976013184
Loss: 2.0616846084594727
Loss: 2.074695587158203
Loss: 2.0789732933044434
Loss: 1.6096206903457642
Loss: 2.475153923034668
Loss: 2.4532017707824707
Loss: 2.4628565311431885
Loss: 2.0563971996307373
Loss: 2.0667147636413574
Loss: 2.8776986598968506
Loss: 2.5406129360198975
Loss: 2.122537136077881
Loss: 2.476062059402466
Loss: 2.244596242904663
Loss: 1.6408801078796387
  0%|          | 0/33 [00:00<?, ?it/s]
finetune_vqarad.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  true_labels = torch.tensor(batch['labels'].nonzero(as_tuple=True)[1])
Test Accuracy:  tensor(0.4581, device='cuda:2')